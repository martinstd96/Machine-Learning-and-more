# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uFYnHn13fUdqqPD3QNHunEsJYCUOPNlP

* https://scikit-learn.org/stable/modules/impute.html
* https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer
* https://scikit-learn.org/stable/auto_examples/impute/plot_iterative_imputer_variants_comparison.html#sphx-glr-auto-examples-impute-plot-iterative-imputer-variants-comparison-py
"""

import pandas as pd
import numpy as np
import warnings
import datetime as dt
import category_encoders as ce

from sklearn.impute import KNNImputer
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import (
    RobustScaler, 
    StandardScaler)

#filtrado de warnings
warnings.filterwarnings('ignore')

# suprimimos la notacion cientifica en los outputs en pandas
pd.options.display.float_format = '{:20,.2f}'.format

#definimos unas constantes
KNN = 'knn'
MEAN = 'mean'
STANDARD = 'standard'
ROBUST = 'robust'

"""### Preproccesings"""

def conversion_features_dataset(dataset):
  if 'dia' in dataset.columns:
    dataset['dia'] = pd.to_datetime(dataset['dia'])
    dataset['anio'] = dataset['dia'].dt.year
    dataset['mes'] = dataset['dia'].dt.month
    dataset['dia'] = dataset['dia'].dt.day

  if 'presion_atmosferica_tarde' in dataset.columns:
    dataset['presion_atmosferica_tarde'] = pd.to_numeric(dataset['presion_atmosferica_tarde'], errors='coerce')

  if 'llovieron_hamburguesas_al_dia_siguiente' in dataset.columns:
    dataset['llovieron_hamburguesas_al_dia_siguiente'] = dataset['llovieron_hamburguesas_al_dia_siguiente'].map(
        {
          'No': 0, 
          'Si': 1, 
          np.NaN: 0
        }
      )

def capitalize_features_categoricos(dataset):
  for columna in obtener_features_categoricos(dataset):
    dataset[columna] = dataset[columna].str.capitalize()

def tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, imputer,  dataset_test=None, test=False):
  imputers = {
    KNN: knn_imputer,
    MEAN: mean_imputer
  }
  
  if test:
    capitalize_features_categoricos(dataset_test)
    conversion_features_dataset(dataset_test)
    
    return imputers[imputer](
      None, 
      None, 
      None,
      dataset_test,
      test
  )

  capitalize_features_categoricos(X_train)
  capitalize_features_categoricos(X_val_dev)
  capitalize_features_categoricos(X_test_holdout)

  conversion_features_dataset(X_train)
  conversion_features_dataset(X_val_dev)
  conversion_features_dataset(X_test_holdout)

  return imputers[imputer](
      X_train, 
      X_val_dev, 
      X_test_holdout
  )

def preprocessing_knn_imputer_robust_escaler_one_hot_encoding_hashing_trick(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):
    
  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, ROBUST, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)

  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN)

  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, ROBUST)

  X_train_sin_barrio = hashing_trick_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = hashing_trick_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_knn_imputer_standar_escaler_one_hot_encoding_hashing_trick(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, STANDARD, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, STANDARD)

  X_train_sin_barrio = hashing_trick_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = hashing_trick_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_knn_imputer_robust_escaler_one_hot_encoding_binary_encoding(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, ROBUST, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, ROBUST)

  X_train_sin_barrio = binary_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = binary_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_knn_imputer_standar_escaler_one_hot_encoding_binary_encoding(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, STANDARD, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, STANDARD)

  X_train_sin_barrio = binary_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = binary_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_mean_imputer_standar_escaler_one_hot_encoding_hashing_trick(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, STANDARD, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio) 
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, MEAN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, STANDARD)

  X_train_sin_barrio = hashing_trick_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = hashing_trick_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_mean_imputer_robust_escaler_one_hot_encoding_hashing_trick(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):
  
  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, ROBUST, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)

  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, MEAN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, ROBUST)

  X_train_sin_barrio = hashing_trick_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = hashing_trick_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_mean_imputer_robust_escaler_one_hot_encoding_binary_encoding(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, ROBUST, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)  
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, MEAN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, ROBUST)

  X_train_sin_barrio = binary_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = binary_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

def preprocessing_mean_imputer_standar_escaler_one_hot_encoding_binary_encoding(X_train, X_val_dev, X_test_holdout, dataset_test=None, test=False):

  if test:
    dataset_test_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, KNN, dataset_test, test)
    dataset_test_escalado = aplicar_sacaler(None, None, STANDARD, dataset_test_sn, True)
    dataset_test_sin_barrio = hashing_trick_encoding(dataset_test_escalado)
    return one_hot_encoding(dataset_test_sin_barrio)
    
  X_train_sn, X_val_dev_sn, X_test_holdout_sn = tratamiento_features_e_imputer(X_train, X_val_dev, X_test_holdout, MEAN)
  
  X_train_scaled, X_val_dev_scaled = aplicar_sacaler(X_train_sn, X_val_dev_sn, STANDARD)

  X_train_sin_barrio = binary_encoding(X_train_scaled)
  X_train_enc = one_hot_encoding(X_train_sin_barrio)
    
  X_val_dev_sin_barrio = binary_encoding(X_val_dev_scaled)
  X_val_dev_enc = one_hot_encoding(X_val_dev_sin_barrio)

  return X_train_enc, X_val_dev_enc, X_test_holdout_sn

"""### Encoders"""

def one_hot_encoding(dataset):
  features_categoricos = obtener_features_categoricos(dataset)
  dataset_categoricos = dataset[features_categoricos]
  dummies = pd.get_dummies(dataset_categoricos)
  dataset_sin_categoricas = dataset.drop(columns=features_categoricos)
  dataset_sin_categoricas[dummies.columns] = dummies
  return dataset_sin_categoricas

def binary_encoding(dataset):
  features_categoricos = obtener_features_categoricos(dataset)
  
  if 'barrio' not in features_categoricos:
    return dataset

  barrios = dataset['barrio'].to_frame()
  encoder = ce.BinaryEncoder(cols=['barrio'])
  barrios_ = barrios.copy()
    
  df_binario = encoder.fit_transform(barrios_)

  dataset_sin_barrio = dataset.drop(columns='barrio')
  dataset_sin_barrio[df_binario.columns] = df_binario
  return dataset_sin_barrio

def hashing_trick_encoding(dataset, target=False):
  features_categoricos = obtener_features_categoricos(dataset)

  if 'barrio' not in features_categoricos:
    return dataset

  barrios = dataset['barrio'].to_frame()
  encoder = ce.HashingEncoder()
  barrios_ = barrios.copy()
    
  df_hash = encoder.hashing_trick(X_in=barrios_, cols=['barrio'], hashing_method='sha256', N=len(features_categoricos))

  dataset_sin_barrio = dataset.drop(columns='barrio')
  dataset_sin_barrio[df_hash.columns] = df_hash
  return dataset_sin_barrio
 
"""### Imputers"""

def obtener_columnas_a_no_imputar():
  columnas_a_no_imputar = ['barrio', 'dia', 'mes', 'anio']
  features_categoricos = ['direccion_viento_tarde', 'direccion_viento_temprano', 'llovieron_hamburguesas_hoy', 'rafaga_viento_max_direccion']

  columnas_a_no_imputar.extend(features_categoricos)

  return columnas_a_no_imputar

def knn_imputer(X_train, X_val_dev, X_hold_out, dataset_test=None, test=False):
  columnas_sin_nulls = obtener_columnas_a_no_imputar()
    
  # definimos un n arbitrario
  imputer = KNNImputer(n_neighbors=2, weights="uniform")
    
  if test:
    df = dataset_test.drop(columns=columnas_sin_nulls)
    df_ = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
    df = df.reset_index()
    df_['id'] = df['id']
    df_ = df_.set_index('id')
    df_[columnas_sin_nulls] = dataset_test[columnas_sin_nulls]
    return df_

  # Eliminamos los features sobre los cuales no hay que imputar nada.
  df_X_train = X_train.drop(columns=columnas_sin_nulls)
  df_X_val_dev = X_val_dev.drop(columns=columnas_sin_nulls)
  df_X_hold_out = X_hold_out.drop(columns=columnas_sin_nulls)

  for feature in df_X_train.columns:
    imputer.fit(df_X_train[feature].values.reshape(-1, 1))
    df_X_train[feature] = imputer.transform(df_X_train[feature].values.reshape(-1, 1))
    df_X_val_dev[feature] = imputer.transform(df_X_val_dev[feature].values.reshape(-1, 1))
    df_X_hold_out[feature] = imputer.transform(df_X_hold_out[feature].values.reshape(-1, 1))

  df_X_train[columnas_sin_nulls] = X_train[columnas_sin_nulls]
    
  df_X_val_dev[columnas_sin_nulls] = X_val_dev[columnas_sin_nulls]

  df_X_hold_out[columnas_sin_nulls] = X_hold_out[columnas_sin_nulls]

  return df_X_train, df_X_val_dev, df_X_hold_out

def mean_imputer(X_train, X_val_dev, X_hold_out, dataset_test=None, test=False):
  columnas_sin_nulls = obtener_columnas_a_no_imputar()

  if test:
    df = dataset_test.drop(columns=columnas_sin_nulls)
    
    for feature in df.columns:
      df[feature].fillna((df[feature].mean()), inplace=True)
        
    df[columnas_sin_nulls] = dataset_test[columnas_sin_nulls]
    
    return df
    
  df_X_train = X_train.drop(columns=columnas_sin_nulls)
  df_X_val_dev = X_val_dev.drop(columns=columnas_sin_nulls)
  df_X_hold_out = X_hold_out.drop(columns=columnas_sin_nulls)

  for feature in df_X_train.columns:
    df_X_train[feature].fillna((df_X_train[feature].mean()), inplace=True)
    df_X_val_dev[feature].fillna((df_X_train[feature].mean()), inplace=True)
    df_X_hold_out[feature].fillna((df_X_train[feature].mean()), inplace=True)
        
  df_X_train[columnas_sin_nulls] = X_train[columnas_sin_nulls]

  df_X_val_dev[columnas_sin_nulls] = X_val_dev[columnas_sin_nulls]
    
  df_X_hold_out[columnas_sin_nulls] = X_hold_out[columnas_sin_nulls]

  return df_X_train, df_X_val_dev, df_X_hold_out

"""### Scalers"""

def aplicar_sacaler(X_train, X_val_dev, tipo_scaler, dataset_test=None, test=False):
  scalers = {STANDARD:StandardScaler,
            ROBUST:RobustScaler} 

  no_categoricos = obtener_features_no_categoricos(X_train)
  categoricos = obtener_features_categoricos(X_train)

  scaler = scalers[tipo_scaler]()

  if test:
    df = dataset_test[no_categoricos]
    aux_ = df.reset_index()
    aux = aux_.drop(labels='id', axis=1)
    scaler.fit(aux)
    scaled_dataset = scaler.transform(aux)
    nombres = aux.columns
    scaled_dataset =  pd.DataFrame(scaled_dataset, columns=nombres)
    scaled_dataset['id'] = aux_['id']
    scaled_dataset[categoricos] = dataset_test[categoricos]
    return scaled_dataset.set_index('id')

  df_X_train = X_train[no_categoricos]
  df_X_val_dev = X_val_dev[no_categoricos]
  aux_X_train_ = df_X_train.reset_index()
  aux_X_val_dev_ = df_X_val_dev.reset_index()
  aux_X_train = aux_X_train_.drop(labels='id', axis=1)
  aux_X_val_dev = aux_X_val_dev_.drop(labels='id', axis=1)

  scaler.fit(aux_X_train)
  scaled_dataset_X_train = scaler.transform(aux_X_train)
  scaled_dataset_X_val_dev = scaler.transform(aux_X_val_dev)

  nombres = aux_X_train.columns
  scaled_dataset_X_train =  pd.DataFrame(scaled_dataset_X_train, columns=nombres)
  scaled_dataset_X_val_dev = pd.DataFrame(scaled_dataset_X_val_dev, columns=nombres)
  scaled_dataset_X_train['id'] = aux_X_train_['id']
  scaled_dataset_X_val_dev['id'] = aux_X_val_dev_['id']
  scaled_dataset_X_train[categoricos] = X_train[categoricos]
  scaled_dataset_X_val_dev[categoricos] = X_val_dev[categoricos]
  return scaled_dataset_X_train.set_index('id'), scaled_dataset_X_val_dev.set_index('id')

"""### Funciones útiles"""

def obtener_features_categoricos(dataset):
  features_categoricos = []

  for columna in dataset.columns:
    if dataset[columna].dtype == np.object: 
      features_categoricos.append(columna)
  
  return features_categoricos

def obtener_features_no_categoricos(dataset):
  features_no_categoricos = []
  features_categoricos = obtener_features_categoricos(dataset)

  for columna in dataset.columns:
    if columna not in features_categoricos:
      features_no_categoricos.append(columna)
  
  return features_no_categoricos

def descargar_datasets(url_values, url_target, url_test):
  csv_format = '/export?format=csv'
  url_values += csv_format
  url_target += csv_format
  url_test += csv_format

  dataset_values = pd.read_csv(url_values, index_col='id')
  dataset_target = pd.read_csv(url_target, index_col='id')
  dataset_test = pd.read_csv(url_test, index_col='id')

  return dataset_values, dataset_target, dataset_test